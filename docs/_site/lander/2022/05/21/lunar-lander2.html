<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Lunar Lander - Part 2 | ML Musings</title>
<meta name="generator" content="Jekyll v3.9.2" />
<meta property="og:title" content="Lunar Lander - Part 2" />
<meta name="author" content="Daniel Mogilevsky" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Introduction In the previous post, we provided an overview of the lunar lander environment and got our solution installed and running. In this post, we’ll talk about the structure of the solution and what’s happening as we train our agent." />
<meta property="og:description" content="Introduction In the previous post, we provided an overview of the lunar lander environment and got our solution installed and running. In this post, we’ll talk about the structure of the solution and what’s happening as we train our agent." />
<link rel="canonical" href="http://localhost:4000/lander/2022/05/21/lunar-lander2.html" />
<meta property="og:url" content="http://localhost:4000/lander/2022/05/21/lunar-lander2.html" />
<meta property="og:site_name" content="ML Musings" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-05-21T14:16:40-04:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Lunar Lander - Part 2" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Daniel Mogilevsky"},"dateModified":"2022-05-21T14:16:40-04:00","datePublished":"2022-05-21T14:16:40-04:00","description":"Introduction In the previous post, we provided an overview of the lunar lander environment and got our solution installed and running. In this post, we’ll talk about the structure of the solution and what’s happening as we train our agent.","headline":"Lunar Lander - Part 2","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/lander/2022/05/21/lunar-lander2.html"},"url":"http://localhost:4000/lander/2022/05/21/lunar-lander2.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="ML Musings" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">ML Musings</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Lunar Lander - Part 2</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2022-05-21T14:16:40-04:00" itemprop="datePublished">May 21, 2022
      </time>• <span itemprop="author" itemscope itemtype="http://schema.org/Person"><span class="p-author h-card" itemprop="name">Daniel Mogilevsky</span></span></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <h3>Introduction</h3>
<p>In the previous post, we provided an overview of the lunar lander environment and got our solution
installed and running. In this post, we’ll talk about the structure of the solution and what’s happening as we train
our agent.</p>

<h3>Structure</h3>
<p>By now you’ve probably taken note of the files in the project directory:</p>
<ul>
  <li>Constants.py: Stores directory paths and strings we use throughout the project</li>
  <li>main.py: Starting point for the project that calls the necessary functions based on user inputs</li>
  <li>eagle_large.py: File with all the AI logic and functions for training, recording videos, saving models, etc</li>
</ul>

<p>Let’s dive into how the learning agent was created</p>
<h3>Creating the learning agent</h3>
<p>Our agent is responsible for fine tuning the model used to fly the lander, below are the parts of the agent
we had to define before the training process could start.</p>

<h4>Hyper Parameters</h4>
<p>Hyper parameters are parameters that control the learning process rather than the performance of the model itself. 
The following hyper parameters must be decided when creating our learning agent:</p>

<ul>
  <li>Learning rate (from 0-1): This determines how quickly the agent will pick up new values</li>
  <li>Gamma a.k.a Discount factor (0-1): How much weight is given to the reward of future actions when calculating the value
of a certain action</li>
  <li>Epsilon decay, initial, and minimum: Epsilon is the probability that we choose a random action for the current frame instead of a 
optimal action. Epsilon decay is the value by which epsilon decreases each frame.</li>
  <li>Memory store: How large of a memory store we want to keep. In our agents memory, we store a state, action, reward for the
state+action, the resulting state, and whether this action ended the episode.</li>
</ul>

<p>Through researching how other people have solved this environment the following hyper parameters were discovered to be optimal:
<a href="https://github.com/gryslik/ml-musings/blob/109d54e39476636e714b686a1c63ef71da54d1ae/lunar_lander/eagle_large.py#L16-L25">Code Link</a></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">DQN</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">env</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">env</span> <span class="o">=</span> <span class="n">env</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">memory</span> <span class="o">=</span> <span class="n">collections</span><span class="p">.</span><span class="n">deque</span><span class="p">(</span><span class="n">maxlen</span><span class="o">=</span><span class="mi">1000000</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="mf">0.99</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">epsilon</span> <span class="o">=</span> <span class="mf">1.0</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">epsilon_min</span> <span class="o">=</span> <span class="mf">0.01</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">epsilon_decay</span> <span class="o">=</span> <span class="mf">0.996</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.001</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">model</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">create_model</span><span class="p">()</span> <span class="c1"># Will do the actual predictions
</span></code></pre></div></div>
<p>In a later blog post, we’ll discuss why these particular parameters are optimal. For now, it’s sufficient to understand
what they are and how they impact our agent.</p>

<h4>Model Structure - Neural Net</h4>
<p>The agent itself doesn’t fly the lander, and the hyper parameters just inform how the agent trains itself, the <em>model</em>
is what actually flies the lander. The model is a part of the agent and something the agent fine-tunes over time. 
We are using a neural net for our model. A neural net consists of an input, hidden, and output layer
and takes input (the agent’s observations about the environment) and turns them into outputs (actions).
We’ll discuss how exactly a neural net works later on, for now it is sufficient to understand the above. 
For our neural net, we are using a <a href="https://keras.io/guides/sequential_model/">Keras Sequential Model</a></p>

<p>We have found the following structure to work well:</p>
<ul>
  <li>Input layer with 8 nodes representing the observation space ((x and y coordinates of the lander, x and y linear velocities, angle, angular velocity, two booleans representing the legs
touching the ground))</li>
  <li>Hidden layer with 150 nodes</li>
  <li>Hidden layer with 120 nodes</li>
  <li>Output layer with 4 nodes representing the action space (do nothing, fire left engine, fire main engine, fire right engine)</li>
</ul>

<p>Below is the code for this initialization
<a href="https://github.com/gryslik/ml-musings/blob/109d54e39476636e714b686a1c63ef71da54d1ae/lunar_lander/eagle_large.py#L27-L34">Code Link</a></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">create_model</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">Sequential</span><span class="p">()</span>
        <span class="n">state_shape</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">env</span><span class="p">.</span><span class="n">observation_space</span><span class="p">.</span><span class="n">shape</span>
        <span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">150</span><span class="p">,</span> <span class="n">input_dim</span> <span class="o">=</span> <span class="n">state_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">activation</span> <span class="o">=</span> <span class="s">"relu"</span><span class="p">))</span>
        <span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">120</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">"relu"</span><span class="p">))</span>
        <span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">env</span><span class="p">.</span><span class="n">action_space</span><span class="p">.</span><span class="n">n</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">"linear"</span><span class="p">))</span>
        <span class="n">model</span><span class="p">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s">"mean_squared_error"</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">optimizers</span><span class="p">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">learning_rate</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">model</span>
</code></pre></div></div>

<p>Once again, don’t sweat the details right now, in our next post, we’ll do that in the final post.</p>

<p>Lastly, our class has two helper functions, <em>act</em> and <em>remember</em></p>

<p><em>Act</em>, shown below, is causes the agent to take an action given a game/environment state. The details of how epsilon
is used will be made clear in the last post, but astute readers will see that epsilon determines whether the
agent picks a random action or what it believes to be the ideal action.
<a href="https://github.com/gryslik/ml-musings/blob/109d54e39476636e714b686a1c63ef71da54d1ae/lunar_lander/eagle_large.py#L63-L69">Code Link</a></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">def</span> <span class="nf">act</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">epsilon</span> <span class="o">*=</span> <span class="bp">self</span><span class="p">.</span><span class="n">epsilon_decay</span> <span class="c1"># Multiply our epsilon by the decay
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">epsilon</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">epsilon_min</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">epsilon</span><span class="p">)</span> <span class="c1"># Never let epsilon go below the minium value
</span>        <span class="k">if</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">random</span><span class="p">()</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="p">.</span><span class="n">epsilon</span><span class="p">:</span> <span class="c1"># Generate a random number 0-1, if it's less than episolon, do a random action
</span>            <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">env</span><span class="p">.</span><span class="n">action_space</span><span class="p">.</span><span class="n">sample</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span> <span class="c1"># Otherwise, pick what we believe to be the best action
</span>            <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">state</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>
</code></pre></div></div>

<p><em>Remember</em> causes the agent to remember the previous state, the action it took in that state, the reward of the action,
the resulting/new state from that action, and whether the game reached completion from the action. It remembers this
by appending all of these details into the memory buffer
<a href="https://github.com/gryslik/ml-musings/blob/109d54e39476636e714b686a1c63ef71da54d1ae/lunar_lander/eagle_large.py#L37">Code Link</a></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">def</span> <span class="nf">remember</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">new_state</span><span class="p">,</span> <span class="n">done</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">memory</span><span class="p">.</span><span class="n">append</span><span class="p">([</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">new_state</span><span class="p">,</span> <span class="n">done</span><span class="p">])</span>
</code></pre></div></div>

<h3>Training the agent</h3>

<p>If you followed the instructions on the previous blog post and have the training running in the background,
you’ve likely taken note of the output and perhaps even saw an instance or two of the lander in action.</p>

<p>Let’s dive into what’s happening in the training cycle.</p>

<p>We started by creating the environment, picking the number of episodes we’ll train the agent for, initializing
the agent, and creating variables to track total reward and # of steps per episode</p>

<p><a href="https://github.com/gryslik/ml-musings/blob/109d54e39476636e714b686a1c63ef71da54d1ae/lunar_lander/eagle_large.py#L155-L196">Link to Code</a></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">train_agent</span><span class="p">():</span>
    <span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="p">.</span><span class="n">make</span><span class="p">(</span><span class="s">'LunarLander-v2'</span><span class="p">)</span>
    <span class="n">num_episodes</span> <span class="o">=</span> <span class="mi">500</span>
    <span class="n">my_agent</span> <span class="o">=</span> <span class="n">DQN</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="n">env</span><span class="p">)</span>
    <span class="n">totalreward</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">steps</span> <span class="o">=</span> <span class="p">[]</span>
</code></pre></div></div>

<p>For each episode, we start at step 0 with a fresh environment and an accumulated reward of 0.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">for</span> <span class="n">episode</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_episodes</span><span class="p">):</span>
        <span class="k">print</span><span class="p">(</span><span class="s">"======================================================"</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="s">"Processing episode: "</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">episode</span><span class="p">))</span>
        <span class="k">print</span><span class="p">(</span><span class="s">"======================================================"</span><span class="p">)</span>
        <span class="n">time_start</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span>
        <span class="n">cur_state</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">reset</span><span class="p">().</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">8</span><span class="p">)</span> <span class="c1"># Reset the environment
</span>        <span class="n">episode_reward</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">step</span> <span class="o">=</span> <span class="mi">0</span>
</code></pre></div></div>

<p>Until the episode terminates, the following cycle occurs:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
</code></pre></div></div>
<ol>
  <li>The agent decides on an action based on the current state.
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">action</span> <span class="o">=</span> <span class="n">my_agent</span><span class="p">.</span><span class="n">act</span><span class="p">(</span><span class="n">cur_state</span><span class="p">)</span>
</code></pre></div>    </div>
  </li>
  <li>We take the action and get back a new state, the reward for the action, and whether we are done with the episode(as per the episode
termination conditions mentioned before).
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">new_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
<span class="n">new_state</span> <span class="o">=</span> <span class="n">new_state</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>
</code></pre></div>    </div>
  </li>
  <li>We make the agent remember the action it took on the previous state and
the result of that action
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">my_agent</span><span class="p">.</span><span class="n">remember</span><span class="p">(</span><span class="n">cur_state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">new_state</span><span class="p">,</span> <span class="n">done</span><span class="p">)</span>
</code></pre></div>    </div>
  </li>
  <li>The agent replays this event to learn from it.
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">my_agent</span><span class="p">.</span><span class="n">replay</span><span class="p">()</span>
</code></pre></div>    </div>
  </li>
  <li>We update environment state, add the reward of the action taken to the total reward, and increment the step.
We also check if we are done with this episode.
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>         <span class="n">cur_state</span> <span class="o">=</span> <span class="n">new_state</span>
         <span class="n">episode_reward</span> <span class="o">+=</span> <span class="n">reward</span>
         <span class="n">step</span> <span class="o">+=</span><span class="mi">1</span>
         <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
             <span class="k">break</span>
</code></pre></div>    </div>
  </li>
</ol>

<p>These values are what you see printed to the console when you train the agent.</p>

<p>Here’s how the models progressed as we trained them:</p>

<p>First attempt (model 0) goes terribly
<img src="/videos/lander_crash.gif" alt="crash" /></p>

<p>By model 40, the lander learns how to hover above the goal area without crashing:
<img src="/videos/lander_hover.gif" alt="hover" /></p>

<p>At model 110, the lander is landing, but not quite in the goal post
<img src="/videos/almost.gif" alt="almost" /></p>

<p>By model/episode 140, the lander is landing in the goal area perfectly:
<img src="/videos/perfect.gif" alt="perfect" /></p>

<h3>Conclusion</h3>

<p>After only 140 episodes of training, our AI can play lunar lander better than
most human players. In our final blog post, we’ll analyze why our hyper parameters and neural net structure work
and the math behind them.</p>

  </div><a class="u-url" href="/lander/2022/05/21/lunar-lander2.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">ML Musings</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">ML Musings</li><li><a class="u-email" href="mailto:gryslik@gmail.com">gryslik@gmail.com</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/gryslik"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">gryslik</span></a></li><li><a href="https://www.twitter.com/jekyllrb"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">jekyllrb</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>An exploration of machine learning</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>

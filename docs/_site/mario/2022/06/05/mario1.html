<h2> Introduction </h2>
<p>Having completed the Lunar Lander series, we are now moving on to a more challenging game, Mario.</p>

<p><img src="/videos/mario_display.gif" alt="Mario Game" /></p>

<p>Unlike the previous project, Lunar Lander, Mario will involve an amount of training time unfeasible for your personal
machine. This blog post will assume you’ve read through the Lunar Lander post and are familiar with how a neural net works
and what the different hyper-parameters do. It will not include a detailed walkthrough of setting up the project
locally, since the reader is unlikely to benefit from doing so.</p>

<h2> Environment </h2>

<p>The <a href="https://pypi.org/project/gym-super-mario-bros/">Mario Environment</a> we are using has the following main objectives,
moving as far to the right as possible and reaching the flag. The aim is to do this as fast as possible without dying.
This is accomplished by tracking the x position of mario in the game and the in game clock. 
The are also various mario stages available, as well as downsampling of stages. We will be training mario on the
first stage without any downsampling.</p>

<p>Unlike Lunar Lander, the Mario agent uses a special kind of neural net called a convolutional neural net.</p>

<h2>Convolutional Neural Network (CNN) </h2>

<p>A CNN is a type of neural net used analyze visual imagery. In Lunar Lander, the environment gave us some explicit 
information about the environment state, such as the location of the lander and its velocity. In Mario, we will be
analyzing the actual game frames (images) similar to a human player to make decisions. This is done by passing each frame
as image data into a CNN, and using the output as action. A detailed representation of this process is shown below:</p>

<p><img src="/images/cnn.png" alt="CNN" /></p>

<h2>Mario Project Learning Agent</h2>

<p>Overall, the agent is very similar in structure to the Lunar Lander one, however. The main differences are the
neural net structure and hyper-parameters. Below, the neural net is created which has 3 convolutional layers
and takes as input the image of the mario game. After the convolutional layers, the data is flattened (meaning has its
dimensions reduced) and is then passed into a regular neural net with a node count of 512 and an output layer representing
the action space.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">def</span> <span class="nf">create_model</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">Sequential</span><span class="p">()</span>
        <span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">single_frame_dim</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="p">.</span><span class="n">single_frame_dim</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_frames_to_stack</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s">"relu"</span><span class="p">))</span>
        <span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">"relu"</span><span class="p">))</span>
        <span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">"relu"</span><span class="p">))</span>
        <span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Flatten</span><span class="p">())</span>
        <span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">"relu"</span><span class="p">))</span>
        <span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">env</span><span class="p">.</span><span class="n">action_space</span><span class="p">.</span><span class="n">n</span><span class="p">))</span>
        <span class="n">model</span><span class="p">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">losses</span><span class="p">.</span><span class="n">Huber</span><span class="p">(),</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">optimizers</span><span class="p">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">learning_rate</span><span class="p">))</span> <span class="c1">## huber loss takes advantage of l1/l2
</span>        <span class="k">return</span> <span class="n">model</span>
</code></pre></div></div>

<p>The hyper-parameters are also different.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">DQN</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span> <span class="n">single_frame_dim</span><span class="p">,</span>  <span class="n">num_frames_to_stack</span><span class="p">,</span> <span class="n">old_model_filepath</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">old_epsilon_value</span> <span class="o">=</span> <span class="bp">None</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">env</span> <span class="o">=</span> <span class="n">env</span>

        <span class="c1">#self.memory = collections.deque(maxlen=10000) #this is slow. Using my custom class.
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">burnin</span> <span class="o">=</span> <span class="mi">3000</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">memory</span> <span class="o">=</span> <span class="n">ReplayMemory</span><span class="p">(</span><span class="n">max_size</span><span class="o">=</span><span class="mi">30000</span><span class="p">)</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="mf">0.9</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">epsilon</span> <span class="o">=</span> <span class="mf">1.0</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">epsilon_min</span> <span class="o">=</span> <span class="mf">0.02</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">epsilon_decay</span> <span class="o">=</span> <span class="mf">0.999995</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.00025</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">update_target_step_count</span> <span class="o">=</span> <span class="mi">5000</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">num_steps_since_last_update</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">single_frame_dim</span> <span class="o">=</span> <span class="n">single_frame_dim</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">num_frames_to_stack</span> <span class="o">=</span> <span class="n">num_frames_to_stack</span>


        <span class="k">if</span><span class="p">(</span><span class="n">old_model_filepath</span><span class="o">==</span><span class="bp">None</span><span class="p">):</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">model</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">create_model</span><span class="p">()</span>  <span class="c1"># Will do the actual predictions
</span>            <span class="bp">self</span><span class="p">.</span><span class="n">target_model</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">create_model</span><span class="p">()</span>  <span class="c1"># Will compute what action we DESIRE from our model
</span>        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">models</span><span class="p">.</span><span class="n">load_model</span><span class="p">(</span><span class="n">old_model_filepath</span><span class="p">)</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">target_model</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">models</span><span class="p">.</span><span class="n">load_model</span><span class="p">(</span><span class="n">old_model_filepath</span><span class="p">)</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">epsilon</span> <span class="o">=</span> <span class="n">old_epsilon_value</span>
</code></pre></div></div>

<p>The training logic can be found in Mario_Trainer.py. It’s also very similar to the lunar lander code.</p>

<h2> Training the models </h2>

<p>Here’s the training progress:</p>

<p>Episode 0, where we only move right:</p>

<p><img src="/videos/mario-0.gif" alt="Mario 0" /></p>

<p>Episode 500, where we’ve learned nothing:</p>

<p><img src="/videos/mario-500.gif" alt="Mario 500" /></p>

<p>Episode 1500, now we can jump:</p>

<p><img src="/videos/mario-1500.gif" alt="Mario 1500" /></p>

<p>Episode 3000, we’re better at jumping but we get stuck against walls:</p>

<p><img src="/videos/mario-3000.gif" alt="Mario 3000" /></p>

<p>Episode 5000, even better at jumping but we still get stuck:</p>

<p><img src="/videos/mario-5000.gif" alt="Mario 5000" /></p>

<p>Episode 7000 Victory!:</p>

<p><img src="/videos/mario-7000.gif" alt="Mario 7000" /></p>

<p>Although there were many successes along the way, the model continued to fail until becoming more consistent around
the 7000 mark.</p>

<p>But what happens if we try to use this model on a new mario level? What degree of success will we have then?</p>

<p>The results are…atrocious</p>

<p><img src="/videos/mario-7000-2.gif" alt="Mario 7000" /></p>

<p><img src="/videos/mario-7000-3.gif" alt="Mario 7000" /></p>

<p>Clearly, our model only works on the level it was trained on. We have overtrained our model to the specific mario level 
we used, and this prevents our model from actually performing well at Mario. In future blog posts, we’ll explore
why this happened and how we can generate a model that can beat all levels of Mario. Stay tuned</p>

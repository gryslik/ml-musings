<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2022-06-29T19:03:59-04:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">ML Musings</title><subtitle>An exploration of machine learning</subtitle><entry><title type="html">Part 3</title><link href="http://localhost:4000/lander/2022/06/05/lander3.html" rel="alternate" type="text/html" title="Part 3" /><published>2022-06-05T14:16:40-04:00</published><updated>2022-06-05T14:16:40-04:00</updated><id>http://localhost:4000/lander/2022/06/05/lander3</id><content type="html" xml:base="http://localhost:4000/lander/2022/06/05/lander3.html"><![CDATA[<p>In part two of the lunar lander series, we showcased the basics of how our AI agent trains and creates models
for flying the lander successfully, however, lots of questions were left unanswered about our design choices. This
third and final post will answer the why behind our hyper parameter and architecture choices.</p>

<h3>Background knowledge</h3>
<h4>Neural net</h4>
<p>In post 2, we showcased the neural net structure and left it at that. Now let’s dive into how a neural net works.
The below image gives a visual representation of what you already know.</p>

<p><img src="/images/neuralnetsimple.png" alt="Neural net Simple" /></p>

<p>Data that gets passed into an input layer,
some hidden layers transform the input, and we get an output through the output layer. 
But how does the data get passed through each node? The diagram below illustrates that process for each node.</p>

<p><img src="/images/neuralnet1node.png" alt="Neural net detailed" /></p>

<p>Each layer of the net is connected to the previous layer, and each node-to-node connection has a certain <em>weight</em>.
For each node, the previous nodes connecting to it are the <em>inputs</em>. Every input gets multiplied by a weight and has a <em>bias</em>
added to it, and then they all get summed up. This is the <em>weighted sum</em>.
The weighted sum is passed into an <em>activation function</em> which returns an output. This output is the value 
of a single node in the next layer.</p>

<p>There are a couple types of activation functions, some of the most common being linear, relu, and sigmoid. All they
do is given a value x, pass it into a function that returns a new value. For example f(x) = k * x is a linear 
activation function that returns x multiplied by some constant k. Relu and sigmoid are just more complex, nonlinear functions.</p>

<p>So essentially:</p>
<ol>
  <li>Input layer values are set by the data (observations)</li>
  <li>Each node in the next layer calculates its value by getting the weighted sum of all the nodes in the previous layer
and passing the weighted sum through an activation function</li>
  <li>Repeat step 2 until we’re out of the neural net</li>
</ol>

<p>This happens every single frame of lunar lander. An observation about the environment gets passed in, and the neural net
spits out an action through the above process. But how does the agent optimize the neural net?</p>

<h4>Action -&gt; Response -&gt; Reward</h4>
<p>In reinforcement learning, this is the basic learning cycle. As displayed below, an agent takes an action and
predicts the reward of this action.
The action produces a response from the environment, with both a new state and actual reward. The agent compares its predicted reward 
to the actual reward. The agent optimizes itself based on the deviation, and takes a new action. This is done by changing
the weights and biases within its neural net through a process called backpropagation. <em>The structure of the neural net never
changes, only the weights and biases change.</em>
This pattern continues until the agent is able to accurately predict the rewards of actions, and then the agent knows
what actions to take.</p>

<p><img src="/images/rlcycle.png" alt="Reinforcement Learning Cycle" /></p>

<p>You may remember a hyper parameter we defined in the last post, epsilon, alongside minimum epsilon and epsilon decay.
Epsilon ranges from 0-1 and is the probability that we choose a random action over what we think will be best. When we
start the agent, it has a high epsilon and thus chooses mostly random values, but tries to predict the reward each time.
As it makes predictions and sees the results, it fine tunes its ability to make predictions. This is done by changing 
the weights within the neural net. Eventually, it starts choosing what it believes to be the optimum action instead of
a random one, how fast this happens depends on our initial epsilon and epsilon decay.</p>

<p>This can be seen in the below act function, which is responsible for performing and returning an action</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="c1"># Take an action given the current state
</span>    <span class="k">def</span> <span class="nf">act</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">epsilon</span> <span class="o">*=</span> <span class="bp">self</span><span class="p">.</span><span class="n">epsilon_decay</span> <span class="c1"># Multiply our epsilon by the decay
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">epsilon</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">epsilon_min</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">epsilon</span><span class="p">)</span> <span class="c1"># Never let epsilon go below the minium value
</span>        <span class="k">if</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">random</span><span class="p">()</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="p">.</span><span class="n">epsilon</span><span class="p">:</span> <span class="c1"># Generate a random number 0-1, if it's less than episolon, do a random action
</span>            <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">env</span><span class="p">.</span><span class="n">action_space</span><span class="p">.</span><span class="n">sample</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span> <span class="c1"># Otherwise, pick what we believe to be the best action
</span>            <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">state</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>
</code></pre></div></div>

<p>An agent predicts the best action using something called the Bellman Equation</p>

<h4>Bellman Equation</h4>
<p>The <a href="https://www.geeksforgeeks.org/bellman-equation/">Bellman’s Equation</a>
, as said in the linked article, states that the long-term reward of an action is equal to the reward of
the action itself plus the expected reward from the following actions.</p>

<p>To understand this better, let’s use a concrete example from the Lunar Lander scenario. When the lander touches a leg
to the ground, it earns a certain number of points, but when the lander crashes it loses significantly more points.
If the lander tries to land too fast, it will first gain points for touching the ground, and then lose all those points
and many more for crashing. The Bellman’s equation will produce a low reward for attempting to land when
moving too fast because the future reward is highly negative.</p>

<p>The learning agent uses the Bellman Equation to predict the value of the current state.</p>

<p><img src="/images/bellman.png" alt="Bellman Equation" /></p>

<p>Where:</p>
<ul>
  <li>V(s) is the value of state s</li>
  <li>max a denotes the optimal action in the current state</li>
  <li>R(s, a) is the reward for action a in state s</li>
  <li>γ is gamma a.k.a the discount factor (one of our hyper parameters) and ranges from 0-1</li>
  <li>s’ is the next state resulting from taking action a in state s</li>
  <li>V(s’) is the value of the next state</li>
</ul>

<p>To summarize, the value of our current state is equal to the reward of taking the optimal action in the current state
plus the discount factor times the value of the resulting state.</p>

<h3>Understanding optimality of our hyper parameters</h3>
<p>With the background knowledge, we can now infer why our chosen parameters are optimal</p>

<h4>Discount factor</h4>
<p>Discount factor, or gamma, was used in the Bellman Equation to give a weight to the value of future predicted rewards.
This essentially controls how far into the future we look, with a larger discount factor giving us farther vision into the future.
We’ve set our gamma very high, at 0.99. Why? Think about how many frames are in a single game of lunar lander. <em>A lot</em>.
If we only take a couple frames into account when making predictions, there’s no way we’ll build an accurate model. Not a
lot changes in between frames, so maximizing the discount factor ensures we look far enough into the future to make accurate
predictions.</p>

<h4>Epsilon</h4>
<p>Just like gamma needed to be set very high because of the number of frames per game, so does epsilon need to be set high
and epsilon decay needs to be slow. Each individual frame has little impact on the outcome. If epsilon
decay happens quickly, the model will start taking “optimal actions” far before it knows what “optimal” is. This may
cause the agent to get stuck in what’s called a local minima. Essentially, the agent will continue to take similar actions
again and again because it thinks other actions are sub-optimal, even if its current behavior is bad. An example of this
would be cutting the engine completely and letting the lander crash because of the reward from touching the legs.</p>

<p>We prevent this with a high epsilon and slow decay, causing the model to train on lots of random actions before picking
optimal actions.</p>

<h4>Learning rate</h4>
<p>Learning rate needs to be <em>sloooooow</em>, also because of the minimum impact of each frame. We want our agent to slowly adjust
it’s behavior from each frame for this reason. We have many, many frames, each very similar to the last. By setting
a slow learning rate, we prevent the agent from over-adjusting from each action.</p>

<p>“Oh, firing the engine in this frame was better than not firing the engine? Okay, let me fire the main engine for 
EVERY SINGLE FRAME FROM NOW ON”</p>

<p>Bad idea.</p>

<h3>Optimality of neural net design</h3>

<p>With our neural net, 2 things were predetermined based on the environment, that being input and output dimension.
The hidden layers, activation functions, and loss function were things we had to choose.</p>

<p>To review, we have two hidden layers, the first one having 150 and the second one having 120 nodes. Both use
the relu activation function. The output layer uses a linear activation function. The loss function we are using is
means squared error (mse).</p>

<h4>Node and layer counts</h4>
<p>Quite frankly, it is very difficult to say for sure that a particular node/layer count is optimal. It’s very much
an experimental process until the best one is discovered. Having too many layers can create a model that’s overfitted
to the training (meaning it won’t generalize well to environments different than the ones it trained on),
and too few causes the model to be too simple and fail to properly train for any environment.
Two hidden layers worked well, so we stuck with that.</p>

<p>For node count, a commonly used strategy is to gradually decrease the node count in the hidden layers. This is why
we went from 150 to 120. Once again, there are no clear “rules” here. It’s best to experiment and find what works best</p>

<h4>Activation functions</h4>
<p>We used relu as the activation function for the hidden layers, and linear for the output.</p>

<p><em>A relu activation function is the same as linear, except that the minimum output of relu is always 0</em>.</p>

<p>It is one of the most commonly used activation functions and often the default, so we felt no need to deviate.</p>]]></content><author><name>Daniel Mogilevsky, Gregory Ryslik</name></author><category term="Lander" /><summary type="html"><![CDATA[In part two of the lunar lander series, we showcased the basics of how our AI agent trains and creates models for flying the lander successfully, however, lots of questions were left unanswered about our design choices. This third and final post will answer the why behind our hyper parameter and architecture choices.]]></summary></entry><entry><title type="html">Part 2</title><link href="http://localhost:4000/lander/2022/05/21/lunar-lander2.html" rel="alternate" type="text/html" title="Part 2" /><published>2022-05-21T14:16:40-04:00</published><updated>2022-05-21T14:16:40-04:00</updated><id>http://localhost:4000/lander/2022/05/21/lunar-lander2</id><content type="html" xml:base="http://localhost:4000/lander/2022/05/21/lunar-lander2.html"><![CDATA[<h3>Introduction</h3>
<p>In the previous post, we provided an overview of the lunar lander environment and got our solution
installed and running. In this post, we’ll talk about the structure of the solution and what’s happening as we train
our agent.</p>

<h3>Structure</h3>
<p>By now you’ve probably taken note of the files in the project directory:</p>
<ul>
  <li>Constants.py: Stores directory paths and strings we use throughout the project</li>
  <li>main.py: Starting point for the project that calls the necessary functions based on user inputs</li>
  <li>eagle_large.py: File with all the AI logic and functions for training, recording videos, saving models, etc</li>
</ul>

<p>Let’s dive into how the learning agent was created</p>
<h3>Creating the learning agent</h3>
<p>Our agent is responsible for fine tuning the model used to fly the lander, below are the parts of the agent
we had to define before the training process could start.</p>

<h4>Hyper Parameters</h4>
<p>Hyper parameters are parameters that control the learning process rather than the performance of the model itself. 
The following hyper parameters must be decided when creating our learning agent:</p>

<ul>
  <li>Learning rate (from 0-1): This determines how quickly the agent will pick up new values</li>
  <li>Gamma a.k.a Discount factor (0-1): How much weight is given to the reward of future actions when calculating the value
of a certain action</li>
  <li>Epsilon decay, initial, and minimum: Epsilon is the probability that we choose a random action for the current frame instead of a 
optimal action. Epsilon decay is the value by which epsilon decreases each frame.</li>
  <li>Memory store: How large of a memory store we want to keep. In our agents memory, we store a state, action, reward for the
state+action, the resulting state, and whether this action ended the episode.</li>
</ul>

<p>Through researching how other people have solved this environment the following hyper parameters were discovered to be optimal:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">DQN</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">env</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">env</span> <span class="o">=</span> <span class="n">env</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">memory</span> <span class="o">=</span> <span class="n">collections</span><span class="p">.</span><span class="n">deque</span><span class="p">(</span><span class="n">maxlen</span><span class="o">=</span><span class="mi">1000000</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="mf">0.99</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">epsilon</span> <span class="o">=</span> <span class="mf">1.0</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">epsilon_min</span> <span class="o">=</span> <span class="mf">0.01</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">epsilon_decay</span> <span class="o">=</span> <span class="mf">0.996</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.001</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">model</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">create_model</span><span class="p">()</span> <span class="c1"># Will do the actual predictions
</span></code></pre></div></div>
<p>In a later blog post, we’ll discuss why these particular parameters are optimal. For now, it’s sufficient to understand
what they are and how they impact our agent.</p>

<h4>Model Structure - Neural Net</h4>
<p>The agent itself doesn’t fly the lander, and the hyper parameters just inform how the agent trains itself, the <em>model</em>
is what actually flies the lander. The model is a part of the agent and something the agent fine-tunes over time. 
We are using a neural net for our model. A neural net consists of an input, hidden, and output layer
and takes input (the agent’s observations about the environment) and turns them into outputs (actions).
We’ll discuss how exactly a neural net works later on, for now it is sufficient to understand the above. 
For our neural net, we are using a <a href="https://keras.io/guides/sequential_model/">Keras Sequential Model</a></p>

<p>We have found the following structure to work well:</p>
<ul>
  <li>Input layer with 8 nodes representing the observation space ((x and y coordinates of the lander, x and y linear velocities, angle, angular velocity, two booleans representing the legs
touching the ground))</li>
  <li>Hidden layer with 150 nodes</li>
  <li>Hidden layer with 120 nodes</li>
  <li>Output layer with 4 nodes representing the action space (do nothing, fire left engine, fire main engine, fire right engine)</li>
</ul>

<p>Below is the code for this initialization</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">create_model</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">Sequential</span><span class="p">()</span>
        <span class="n">state_shape</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">env</span><span class="p">.</span><span class="n">observation_space</span><span class="p">.</span><span class="n">shape</span>
        <span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">150</span><span class="p">,</span> <span class="n">input_dim</span> <span class="o">=</span> <span class="n">state_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">activation</span> <span class="o">=</span> <span class="s">"relu"</span><span class="p">))</span>
        <span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">120</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">"relu"</span><span class="p">))</span>
        <span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">env</span><span class="p">.</span><span class="n">action_space</span><span class="p">.</span><span class="n">n</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">"linear"</span><span class="p">))</span>
        <span class="n">model</span><span class="p">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s">"mean_squared_error"</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">optimizers</span><span class="p">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">learning_rate</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">model</span>
</code></pre></div></div>

<p>Once again, don’t sweat the details right now, in our next post, we’ll go into detail about what happens
inside a neural net.</p>
<h3>Training the agent</h3>

<p>If you followed the instructions on the previous blog post and have the training running in the background,
you’ve likely taken note of the output and perhaps even saw an instance or two of the lander in action.</p>

<p>Let’s dive into what’s happening in the training cycle.</p>

<p>We started by creating the environment, picking the number of episodes we’ll train the agent for, initializing
the agent, and creating variables to track total reward and # of steps per episode</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">train_agent</span><span class="p">():</span>
    <span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="p">.</span><span class="n">make</span><span class="p">(</span><span class="s">'LunarLander-v2'</span><span class="p">)</span>
    <span class="n">num_episodes</span> <span class="o">=</span> <span class="mi">500</span>
    <span class="n">my_agent</span> <span class="o">=</span> <span class="n">DQN</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="n">env</span><span class="p">)</span>
    <span class="n">totalreward</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">steps</span> <span class="o">=</span> <span class="p">[]</span>
</code></pre></div></div>

<p>For each episode, we start at step 0 with a fresh environment and an accumulated reward of 0.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">for</span> <span class="n">episode</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_episodes</span><span class="p">):</span>
        <span class="k">print</span><span class="p">(</span><span class="s">"======================================================"</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="s">"Processing episode: "</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">episode</span><span class="p">))</span>
        <span class="k">print</span><span class="p">(</span><span class="s">"======================================================"</span><span class="p">)</span>
        <span class="n">time_start</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span>
        <span class="n">cur_state</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">reset</span><span class="p">().</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">8</span><span class="p">)</span> <span class="c1"># Reset the environment
</span>        <span class="n">episode_reward</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">step</span> <span class="o">=</span> <span class="mi">0</span>
</code></pre></div></div>

<p>Until the episode terminates, the following cycle occurs:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
</code></pre></div></div>
<ol>
  <li>The agent decides on an action based on the current state.
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">action</span> <span class="o">=</span> <span class="n">my_agent</span><span class="p">.</span><span class="n">act</span><span class="p">(</span><span class="n">cur_state</span><span class="p">)</span>
</code></pre></div>    </div>
  </li>
  <li>We take the action and get back a new state, the reward for the action, and whether we are done with the episode(as per the episode
termination conditions mentioned before).
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">new_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
<span class="n">new_state</span> <span class="o">=</span> <span class="n">new_state</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>
</code></pre></div>    </div>
  </li>
  <li>We make the agent remember the action it took on the previous state and
the result of that action
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">my_agent</span><span class="p">.</span><span class="n">remember</span><span class="p">(</span><span class="n">cur_state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">new_state</span><span class="p">,</span> <span class="n">done</span><span class="p">)</span>
</code></pre></div>    </div>
  </li>
  <li>The agent replays this event to learn from it.
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">my_agent</span><span class="p">.</span><span class="n">replay</span><span class="p">()</span>
</code></pre></div>    </div>
  </li>
  <li>We update environment state, add the reward of the action taken to the total reward, and increment the step.
We also check if we are done with this episode.
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>         <span class="n">cur_state</span> <span class="o">=</span> <span class="n">new_state</span>
         <span class="n">episode_reward</span> <span class="o">+=</span> <span class="n">reward</span>
         <span class="n">step</span> <span class="o">+=</span><span class="mi">1</span>
         <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
             <span class="k">break</span>
</code></pre></div>    </div>
  </li>
</ol>

<p>These values are what you see printed to the console when you train the agent.</p>

<p>Here’s how the models progressed as we trained them:</p>

<p>First attempt (model 0) goes terribly
<img src="/videos/lander_crash.gif" alt="crash" /></p>

<p>By model 40, the lander learns how to hover above the goal area without crashing:
<img src="/videos/lander_hover.gif" alt="hover" /></p>

<p>At model 110, the lander is landing, but not quite in the goal post
<img src="/videos/almost.gif" alt="almost" /></p>

<p>By model/episode 140, the lander is landing in the goal area perfectly:
<img src="/videos/perfect.gif" alt="perfect" /></p>

<h3>Conclusion</h3>

<p>After only 140 episodes of training, our AI can play lunar lander better than
most human players. In our final blog post, we’ll analyze why our hyper parameters and neural net structure work
and the math behind them.</p>]]></content><author><name>Daniel Mogilevsky, Gregory Ryslik</name></author><category term="Lander" /><summary type="html"><![CDATA[Introduction In the previous post, we provided an overview of the lunar lander environment and got our solution installed and running. In this post, we’ll talk about the structure of the solution and what’s happening as we train our agent.]]></summary></entry><entry><title type="html">Introduction</title><link href="http://localhost:4000/lander/2022/05/20/lunar-lander.html" rel="alternate" type="text/html" title="Introduction" /><published>2022-05-20T14:16:40-04:00</published><updated>2022-05-20T14:16:40-04:00</updated><id>http://localhost:4000/lander/2022/05/20/lunar-lander</id><content type="html" xml:base="http://localhost:4000/lander/2022/05/20/lunar-lander.html"><![CDATA[<h3>Introduction</h3>
<p>In reinforcement learning, an agent (an entity that makes decisions) gets rewarded or 
punished for its actions, and over time optimizes itself to better perform as it learns what actions yield the highest reward.</p>

<p>In this series, we use reinforcement learning to make an intelligent lunar lander playing AI.</p>

<p>The rules of Lunar Lander are simple. The user controls a lander by firing a combination of three thrusters:
down, left, and right. The player must fire these thrusters in a way to safely land within the area marked by the flags.</p>

<p><img src="/images/lunar_lander.png" alt="Lunar Lander Game Guide" /></p>

<h3>Environment</h3>
<p>To play this game with an AI, we will be using the <a href="https://www.gymlibrary.ml/environments/box2d/lunar_lander/">Gym Box2D lunar lander environment</a>. 
This environment takes care of many things for us, such as creating the actual game, its rules, and providing a way
for our AI to play the game. The environment specifies the following rewards and punishments:</p>

<p>Rewards:</p>
<ul>
  <li>Moving from the top of the screen to the landing pad yields 100-140 points, which are lost if the lander moves away from the landing zone</li>
  <li>Each leg that contacts the ground yields 10 points</li>
  <li>If the lander comes to rest, it gets 100 points</li>
</ul>

<p>Punishments:</p>
<ul>
  <li>Firing the down thruster is -0.3 points per frame</li>
  <li>Firing either of the side thrusters is -0.03 points per frame</li>
  <li>Crashing the lander is -100 points</li>
</ul>

<p>Each iteration of the game is an “episode”, and an episode ends when the lander comes to rest, crashes, leaves the screen, or achieves a certain score (200).
If the agent ends the episode with a score of 200 or more, the game has been won.</p>

<p>An episode is split into “frames”, a frame being the smallest time slice in a game (FPS in a video game). Our agent
makes calculations every frame. You’ll see this referenced often.</p>

<p>For more information, visit the Lunar Lander environment link above.</p>

<p>Now that we understand our constraints, let’s get our solution setup and working.</p>

<h3>Setting up the project</h3>

<p>Before installing everything we need, you can do the optional step of installing <a href="https://www.anaconda.com/">Anaconda</a>
and <a href="https://www.machinelearningplus.com/deployment/conda-create-environment-and-everything-you-need-to-know-to-manage-conda-virtual-environment/">creating a conda environment</a>. 
This is recommended, but completely unnecessary, so feel free to skip this step if you are fine with installing python
packages outside of a virtual environment.</p>

<p>Now, the following must be installed for our project to run:</p>
<ul>
  <li><a href="https://www.python.org/downloads/">Python 3</a></li>
  <li><a href="https://numpy.org/install/">Numpy</a></li>
  <li><a href="https://www.tensorflow.org/install/">Tensorflow</a></li>
  <li><a href="https://pypi.org/project/gym/">Gym</a></li>
  <li><a href="https://pypi.org/project/pandas/">Pandas</a></li>
  <li><a href="https://git-scm.com/downloads">Git</a> (Optional, makes it easy to clone the Git repository with all our code)</li>
</ul>

<p>Once you have the above, clone the <a href="https://github.com/gryslik/ml-musings/tree/lunar_lander">Lunar Lander Git repo</a> by
running the following in a terminal</p>

<p><code>git clone git@github.com:gryslik/ml-musings.git</code></p>

<h3>Running the trainer</h3>
<p>To run the project, go into the project directory and from the terminal run:</p>
<p><code>python3 main.py</code></p>

<p>When prompted, enter “1” to start training the agent. This will take a while. Your output should
look like the below and continue for a while</p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>======================================================
Processing episode: 0
======================================================
1/1 [==============================] - 1s 857ms/step
1/1 [==============================] - 0s 60ms/step
1/1 [==============================] - 0s 40ms/step
1/1 [==============================] - 0s 52ms/step
1/1 [==============================] - 0s 23ms/step
1/1 [==============================] - 0s 18ms/step
2022-06-29 18:58:58.907627: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled
2022-06-29 18:58:58.950908: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled
1/1 [==============================] - 0s 17ms/step
1/1 [==============================] - 0s 11ms/step
1/1 [==============================] - 0s 18ms/step
2022-06-29 18:58:59.378810: W tensorflow/core/data/root_dataset.cc:247] Optimization loop failed: CANCELLED: Operation was cancelled
1/1 [==============================] - 0s 14ms/step
1/1 [==============================] - 0s 12ms/step
1/1 [==============================] - 0s 16ms/step
--------------------------------------------------------
Episode: 0 completed in: 78 steps.
--------------------------------------------------------
Failed to complete episode: 0 with a total reward of: -137.2459988101599
Processing episode: 0 took: 3 seconds. Avg running reward is: -137.2459988101599
======================================================
Processing episode: 1
======================================================
</code></pre></div></div>

<h3>While the agent is training, check
out the next blog post which talks about the project structure and what's happening as you're waiting.</h3>]]></content><author><name>Daniel Mogilevsky, Gregory Ryslik</name></author><category term="Lander" /><summary type="html"><![CDATA[Introduction In reinforcement learning, an agent (an entity that makes decisions) gets rewarded or punished for its actions, and over time optimizes itself to better perform as it learns what actions yield the highest reward.]]></summary></entry></feed>